{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E806mLjUYFPx",
        "outputId": "2e71e52f-6e74-47b4-f0f2-8fe741f53242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Rating  Price                      Review\n",
            "0       5    120  Great location, very clean\n",
            "1       2     95         Not worth the price\n",
            "2       4    200  Will definitely stay again\n",
            "3       5    150    This hotel was the bomb!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example travel review data\n",
        "data = {\n",
        "    \"Rating\": [5, 2, 4, 5],\n",
        "    \"Price\": [120, 95, 200, 150],\n",
        "    \"Review\": [\n",
        "        \"Great location, very clean\",\n",
        "        \"Not worth the price\",\n",
        "        \"Will definitely stay again\",\n",
        "        \"This hotel was the bomb!\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average hotel rating\n",
        "average_rating = df['Rating'].mean()\n",
        "print(\"Average rating:\", average_rating)\n",
        "\n",
        "# Calculate the average price\n",
        "average_price = df['Price'].mean()\n",
        "print(\"Average price:\", average_price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3aYhYZ0YjJk",
        "outputId": "d6b4f383-72f3-442c-dd3f-b66cf3c55db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average rating: 4.0\n",
            "Average price: 141.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to calculate the 'average' review (doesn't work!)\n",
        "try:\n",
        "    print(df['Review'].mean())\n",
        "except Exception as e:\n",
        "    print(\"Error when trying to calculate mean of reviews:\", e)"
      ],
      "metadata": {
        "id": "LFU0uF5JYzdj",
        "outputId": "e5f17aed-cd70-4b30-cf9a-035b6cea65cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error when trying to calculate mean of reviews: Could not convert string 'Great location, very cleanNot worth the priceWill definitely stay againThis hotel was the bomb!' to numeric\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def travel_eliza_bot(user_input):\n",
        "    user_input = user_input.lower()\n",
        "    # Rule-based responses\n",
        "    if \"book\" in user_input or \"reservation\" in user_input:\n",
        "        return \"I can help you book your trip! Where do you want to go?\"\n",
        "    elif \"cancel\" in user_input:\n",
        "        return \"I'm sorry to hear that you want to cancel. Which reservation should I look up?\"\n",
        "    elif \"recommend\" in user_input or \"suggest\" in user_input:\n",
        "        return \"Sure! What type of destination are you interested in—beaches, cities, or mountains?\"\n",
        "    elif \"hello\" in user_input or \"hi\" in user_input:\n",
        "        return \"Hello! How can I assist you with your travel plans today?\"\n",
        "    elif \"thank\" in user_input:\n",
        "        return \"You're welcome! Let me know if you have more travel questions.\"\n",
        "    elif \"price\" in user_input or \"cost\" in user_input:\n",
        "        return \"I can check prices for you. Are you looking for flights, hotels, or both?\"\n",
        "    else:\n",
        "        return \"Can you please tell me more about your travel needs?\"\n",
        "\n",
        "# Try out the bot\n",
        "print(\"TravelBot: Hello! How can I assist you with your travel plans today?\")\n",
        "while True:\n",
        "    user = input(\"You: \")\n",
        "    if user.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"TravelBot: Safe travels! Goodbye.\")\n",
        "        break\n",
        "    response = travel_eliza_bot(user)\n",
        "    print(\"TravelBot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "oYBsugSnu4js",
        "outputId": "5994f20d-9103-4a0c-f08e-e3d8e55fb6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TravelBot: Hello! How can I assist you with your travel plans today?\n",
            "You: Can you recommend a hotel in Paris?’\n",
            "TravelBot: Sure! What type of destination are you interested in—beaches, cities, or mountains?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-445950414.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TravelBot: Hello! How can I assist you with your travel plans today?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bye\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TravelBot: Safe travels! Goodbye.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')                  # ← ensure this is installed\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample travel review data\n",
        "data = {\n",
        "    \"Rating\": [5, 2, 4, 5],\n",
        "    \"Price\": [120, 95, 200, 150],\n",
        "    \"Review\": [\n",
        "        \"Great location, very clean!\",\n",
        "        \"Not worth the price.\",\n",
        "        \"Will definitely stay again.\",\n",
        "        \"This hotel was the bomb!!! 😊\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "\n",
        "# Step 1: Lowercase conversion (standardization)\n",
        "df['clean_review'] = df['Review'].str.lower()\n",
        "\n",
        "# Step 2: Remove punctuation and special characters (noise removal)\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)     # Remove punctuation\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII (e.g., emojis)\n",
        "    return text\n",
        "\n",
        "df['clean_review'] = df['clean_review'].apply(remove_noise)\n",
        "\n",
        "# Step 3: Tokenization (split into words)\n",
        "df['tokens'] = df['clean_review'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Step 4: Remove stopwords (common uninformative words)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens_no_stop'] = df['tokens'].apply(\n",
        "    lambda tokens: [w for w in tokens if w not in stop_words]\n",
        ")\n",
        "\n",
        "# Step 5: Lemmatization (reduce words to base form)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized'] = df['tokens_no_stop'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(t) for t in tokens]\n",
        ")\n",
        "\n",
        "# Display the processed DataFrame columns\n",
        "print(\"\\nProcessed Text Data:\")\n",
        "print(df[['Review', 'clean_review', 'tokens', 'tokens_no_stop', 'lemmatized']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jenoVrbMFxCy",
        "outputId": "7b5de142-8dee-420b-bdb9-ee2d5906e43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Rating  Price                        Review\n",
            "0       5    120   Great location, very clean!\n",
            "1       2     95          Not worth the price.\n",
            "2       4    200   Will definitely stay again.\n",
            "3       5    150  This hotel was the bomb!!! 😊\n",
            "\n",
            "Processed Text Data:\n",
            "                         Review                clean_review  \\\n",
            "0   Great location, very clean!   great location very clean   \n",
            "1          Not worth the price.         not worth the price   \n",
            "2   Will definitely stay again.  will definitely stay again   \n",
            "3  This hotel was the bomb!!! 😊    this hotel was the bomb    \n",
            "\n",
            "                            tokens            tokens_no_stop  \\\n",
            "0   [great, location, very, clean]  [great, location, clean]   \n",
            "1         [not, worth, the, price]            [worth, price]   \n",
            "2  [will, definitely, stay, again]        [definitely, stay]   \n",
            "3    [this, hotel, was, the, bomb]             [hotel, bomb]   \n",
            "\n",
            "                 lemmatized  \n",
            "0  [great, location, clean]  \n",
            "1            [worth, price]  \n",
            "2        [definitely, stay]  \n",
            "3             [hotel, bomb]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')                      # Tokenizer\n",
        "nltk.download('punkt_tab')                  # ← ensure this is installed\n",
        "nltk.download('stopwords')                  # Stopwords list\n",
        "nltk.download('wordnet')                    # Lemmatizer dictionary\n",
        "nltk.download('averaged_perceptron_tagger')# POS tagger (optional)\n",
        "\n",
        "# Sample travel review data\n",
        "data = {\n",
        "    \"Rating\": [5, 2, 4, 5],\n",
        "    \"Price\": [120, 95, 200, 150],\n",
        "    \"Review\": [\n",
        "        \"Great location, very clean!\",\n",
        "        \"Not worth the price.\",\n",
        "        \"Will definitely stay again.\",\n",
        "        \"This hotel was the bomb!!! 😊\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Data:\")\n",
        "print(df)\n",
        "\n",
        "# Step 1: Lowercase conversion (standardization)\n",
        "df['clean_review'] = df['Review'].str.lower()\n",
        "\n",
        "# Step 2: Remove punctuation and special characters using str.replace with regex=True\n",
        "df['clean_review'] = df['clean_review'].str.replace(r'[^\\w\\s]', '', regex=True)  # remove punctuation\n",
        "\n",
        "# Step 3: Remove non-ASCII characters (like emojis) using str.replace with regex=True\n",
        "df['clean_review'] = df['clean_review'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
        "\n",
        "# Step 4: Tokenization (split into words)\n",
        "df['tokens'] = df['clean_review'].apply(nltk.word_tokenize)\n",
        "\n",
        "# Step 5: Remove stopwords (common uninformative words)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens_no_stop'] = df['tokens'].apply(lambda tokens: [w for w in tokens if w not in stop_words])\n",
        "\n",
        "# Step 6: Lemmatization (reduce words to base form)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized'] = df['tokens_no_stop'].apply(lambda tokens: [lemmatizer.lemmatize(t) for t in tokens])\n",
        "\n",
        "# Display the processed DataFrame columns\n",
        "print(\"\\nProcessed Text Data:\")\n",
        "print(df[['Review', 'clean_review', 'tokens', 'tokens_no_stop', 'lemmatized']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcHF8wECJhlF",
        "outputId": "b51d6cd2-10ff-435f-a566-80c5fdbf8453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "   Rating  Price                        Review\n",
            "0       5    120   Great location, very clean!\n",
            "1       2     95          Not worth the price.\n",
            "2       4    200   Will definitely stay again.\n",
            "3       5    150  This hotel was the bomb!!! 😊\n",
            "\n",
            "Processed Text Data:\n",
            "                         Review                clean_review  \\\n",
            "0   Great location, very clean!   great location very clean   \n",
            "1          Not worth the price.         not worth the price   \n",
            "2   Will definitely stay again.  will definitely stay again   \n",
            "3  This hotel was the bomb!!! 😊    this hotel was the bomb    \n",
            "\n",
            "                            tokens            tokens_no_stop  \\\n",
            "0   [great, location, very, clean]  [great, location, clean]   \n",
            "1         [not, worth, the, price]            [worth, price]   \n",
            "2  [will, definitely, stay, again]        [definitely, stay]   \n",
            "3    [this, hotel, was, the, bomb]             [hotel, bomb]   \n",
            "\n",
            "                 lemmatized  \n",
            "0  [great, location, clean]  \n",
            "1            [worth, price]  \n",
            "2        [definitely, stay]  \n",
            "3             [hotel, bomb]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re, unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# -----------------------------\n",
        "# NLTK resources (run once)\n",
        "# -----------------------------\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')                  # ← ensure this is installed\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Download the English tagger\n",
        "\n",
        "# -----------------------------\n",
        "# Sample travel reviews\n",
        "# -----------------------------\n",
        "df = pd.DataFrame({\n",
        "    \"Rating\": [5, 2, 4, 5],\n",
        "    \"Price\":  [120, 95, 200, 150],\n",
        "    \"Review\": [\n",
        "        \"Great location, very clean!\",\n",
        "        \"Not worth the price.\",\n",
        "        \"Will definitely stay again.\",\n",
        "        \"This hotel was the bomb!!! 😊\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "# -----------------------------\n",
        "# Config toggles (set to True/False as needed)\n",
        "# -----------------------------\n",
        "REMOVE_DIGITS = True\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "# Full(er) Unicode normalization:\n",
        "# - NFKD decompose, drop combining marks (accents)\n",
        "# - NFKC fold (compatibility)\n",
        "def normalize_unicode(txt: str) -> str:\n",
        "    if not isinstance(txt, str):\n",
        "        return \"\"\n",
        "    nfkd = unicodedata.normalize(\"NFKD\", txt)\n",
        "    no_marks = \"\".join(ch for ch in nfkd if not unicodedata.combining(ch))\n",
        "    return unicodedata.normalize(\"NFKC\", no_marks)\n",
        "\n",
        "# Contraction expansion (extend as needed)\n",
        "_contractions = {\n",
        "    r\"\\bcan't\\b\": \"cannot\",\n",
        "    r\"\\bwon't\\b\": \"will not\",\n",
        "    r\"n't\\b\": \" not\",\n",
        "    r\"'re\\b\": \" are\",\n",
        "    r\"'s\\b\": \" is\",\n",
        "    r\"'d\\b\": \" would\",\n",
        "    r\"'ll\\b\": \" will\",\n",
        "    r\"'t\\b\": \" not\",\n",
        "    r\"'ve\\b\": \" have\",\n",
        "    r\"'m\\b\": \" am\",\n",
        "    r\"\\bit’s\\b\": \"it is\",\n",
        "    r\"\\bit's\\b\": \"it is\",\n",
        "    r\"\\bi’m\\b\": \"i am\",\n",
        "    r\"\\bi'm\\b\": \"i am\",\n",
        "}\n",
        "_contr_re = [(re.compile(p, flags=re.IGNORECASE), r) for p, r in _contractions.items()]\n",
        "\n",
        "def expand_contractions(txt: str) -> str:\n",
        "    out = txt\n",
        "    for cre, repl in _contr_re:\n",
        "        out = cre.sub(repl, out)\n",
        "    return out\n",
        "\n",
        "# POS mapping for WordNet lemmatizer\n",
        "def wn_pos(tag: str):\n",
        "    if tag.startswith(\"J\"): return wordnet.ADJ\n",
        "    if tag.startswith(\"V\"): return wordnet.VERB\n",
        "    if tag.startswith(\"N\"): return wordnet.NOUN\n",
        "    if tag.startswith(\"R\"): return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "# Tiny illustrative naive normalizer (optional, for demo)\n",
        "def naive_normalize_token(t: str) -> str:\n",
        "    for suf in (\"ing\",\"ed\",\"es\",\"s\"):\n",
        "        if len(t) > 4 and t.endswith(suf):\n",
        "            return t[:-len(suf)]\n",
        "    return t\n",
        "\n",
        "# -----------------------------\n",
        "# Pipeline\n",
        "# -----------------------------\n",
        "clean = df.copy()\n",
        "\n",
        "# 1) Unicode normalize + lowercase\n",
        "clean[\"clean_text\"] = clean[\"Review\"].astype(str).apply(normalize_unicode).str.lower()\n",
        "\n",
        "# 2) Contraction expansion\n",
        "clean[\"clean_text\"] = clean[\"clean_text\"].apply(expand_contractions)\n",
        "\n",
        "# 3) Remove punctuation/symbols (vectorized, regex=True)\n",
        "clean[\"clean_text\"] = clean[\"clean_text\"].str.replace(r\"[^\\w\\s]\", \" \", regex=True)\n",
        "\n",
        "# 4) Optional digit removal\n",
        "if REMOVE_DIGITS:\n",
        "    clean[\"clean_text\"] = clean[\"clean_text\"].str.replace(r\"\\d+\", \" \", regex=True)\n",
        "\n",
        "# 5) Collapse multiple spaces and trim\n",
        "clean[\"clean_text\"] = clean[\"clean_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "# 6) Tokenization (NLTK)\n",
        "clean[\"tokens\"] = clean[\"clean_text\"].apply(nltk.word_tokenize)\n",
        "\n",
        "# 7) Stopword removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "clean[\"tokens_no_stop\"] = clean[\"tokens\"].apply(lambda toks: [w for w in toks if w not in stop_words])\n",
        "\n",
        "# 8) POS tagging on the filtered tokens\n",
        "clean[\"pos_tags\"] = clean[\"tokens_no_stop\"].apply(nltk.pos_tag)\n",
        "\n",
        "# 9) Lemmatization with POS\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_with_pos(toks):\n",
        "    tagged = nltk.pos_tag(toks)\n",
        "    return [lemmatizer.lemmatize(w, wn_pos(tag)) for w, tag in tagged]\n",
        "\n",
        "clean[\"lemmatized\"] = clean[\"tokens_no_stop\"].apply(lemmatize_with_pos)\n",
        "\n",
        "# 10) Convenience columns (as in earlier version)\n",
        "clean[\"clean_no_stop\"] = clean[\"tokens_no_stop\"].apply(lambda toks: \" \".join(toks))\n",
        "clean[\"tokens_normalized\"] = clean[\"tokens_no_stop\"].apply(lambda toks: [naive_normalize_token(t) for t in toks])\n",
        "\n",
        "# Final preview\n",
        "print(clean[[\n",
        "    \"Review\",\n",
        "    \"clean_text\",\n",
        "    \"tokens\",\n",
        "    \"tokens_no_stop\",\n",
        "    \"lemmatized\",\n",
        "    \"pos_tags\",\n",
        "    \"clean_no_stop\",\n",
        "    \"tokens_normalized\"\n",
        "]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKaO8QQhR0xs",
        "outputId": "7a46e653-b4f4-4b2e-b053-b398c8858184"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                         Review                  clean_text  \\\n",
            "0   Great location, very clean!   great location very clean   \n",
            "1          Not worth the price.         not worth the price   \n",
            "2   Will definitely stay again.  will definitely stay again   \n",
            "3  This hotel was the bomb!!! 😊     this hotel was the bomb   \n",
            "\n",
            "                            tokens            tokens_no_stop  \\\n",
            "0   [great, location, very, clean]  [great, location, clean]   \n",
            "1         [not, worth, the, price]            [worth, price]   \n",
            "2  [will, definitely, stay, again]        [definitely, stay]   \n",
            "3    [this, hotel, was, the, bomb]             [hotel, bomb]   \n",
            "\n",
            "                 lemmatized                                    pos_tags  \\\n",
            "0  [great, location, clean]  [(great, JJ), (location, NN), (clean, NN)]   \n",
            "1            [worth, price]                  [(worth, JJ), (price, NN)]   \n",
            "2        [definitely, stay]              [(definitely, RB), (stay, VB)]   \n",
            "3             [hotel, bomb]                   [(hotel, NN), (bomb, NN)]   \n",
            "\n",
            "          clean_no_stop         tokens_normalized  \n",
            "0  great location clean  [great, location, clean]  \n",
            "1           worth price            [worth, price]  \n",
            "2       definitely stay        [definitely, stay]  \n",
            "3            hotel bomb             [hotel, bomb]  \n"
          ]
        }
      ]
    }
  ]
}